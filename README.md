# Simple Transformer Implementation

This repository contains a basic implementation of the Transformer architecture, featuring a single encoder and a single decoder with single-head self-attention. This minimal setup is intended for educational purposes and to help users understand the core components of the Transformer model.

## Overview

The Transformer model, introduced in the seminal paper by Vaswani et al., revolutionized natural language processing by leveraging self-attention mechanisms. This implementation focuses on the core principles of the Transformer, including:

- **Single Encoder**: Processes input sequences.
- **Single Decoder**: Generates output sequences.
- **Single-Head Self-Attention**: Attends to different positions within the sequence.

##Notes:
Here provided a basic forward pass, using random numbers as word ID's, please check it out in Data.py 
Training will be a future work, but Data.py makes one understand better about  trasnformer working.


## References

1. [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
2. [Attention is All You Need (Paper)](https://arxiv.org/abs/1706.03762)
3. [Umar Jamil coding Transformers](https://youtu.be/ISNdQcPhsts?si=C1Xj60YPvXhCf-5R)


