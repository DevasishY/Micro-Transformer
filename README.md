# Simple Transformer Implementation

This repository contains a basic implementation of the Transformer architecture, featuring a single encoder and a single decoder with single-head self-attention. This minimal setup is intended for educational purposes and to help users understand the core components of the Transformer model.

## Overview

The Transformer model, introduced in the seminal paper by Vaswani et al., revolutionized natural language processing by leveraging self-attention mechanisms. This implementation focuses on the core principles of the Transformer, including:

- **Single Encoder**: Processes input sequences.
- **Single Decoder**: Generates output sequences.
- **Single-Head Self-Attention**: Attends to different positions within the sequence.


